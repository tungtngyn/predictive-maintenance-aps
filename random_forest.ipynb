{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "This notebook fits a Random Forest model to the Scania Trucks Air Pressure System (APS) predictive maintenance dataset, obtained from [UCI's data repository](https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks). \n",
    "\n",
    "In the Logistic Regression & Naive Bayes notebooks, grid searches were used for hyperparameter tuning. However, significantly more computational resources are required to fit a Random Forest model. Thus, this notebook will use Bayesian Optimization in lieu of grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, make_scorer, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'./data/aps_failure_training_set_data_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df --> X_train & y_train\n",
    "X_train = df_train.drop('class', axis=1)\n",
    "y_train = df_train['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_misclassification_cost(y, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    return 10*fp + 500*fn\n",
    "\n",
    "\n",
    "misclassification_cost = make_scorer(\n",
    "    calc_misclassification_cost,\n",
    "    greater_is_better=False,\n",
    "    needs_proba=False,\n",
    "    needs_threshold=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogFile:\n",
    "    def __init__(self, file_path):\n",
    "        self.i = 1\n",
    "        self.file_path = file_path\n",
    "        pass\n",
    "\n",
    "    def write_log_file(self, cv_mean_cost, params, params_skl):\n",
    "        \n",
    "        # Initialize\n",
    "        dd = defaultdict(list)\n",
    "\n",
    "        # Combine hyperopt params & skl params\n",
    "        for d in (params, params_skl):\n",
    "            for key, value in d.items():\n",
    "                dd[key].append(value)\n",
    "\n",
    "        # Create df & add iteration / cost\n",
    "        df = pd.DataFrame(dd, index=['hyperopt', 'sklearn'])\n",
    "        df['Iteration'] = self.i\n",
    "        df['cv_mean_cost'] = cv_mean_cost\n",
    "\n",
    "        # Reorder & append to log file\n",
    "        df = df[['Iteration', 'cv_mean_cost', *dd.keys()]]\n",
    "\n",
    "        # Write header\n",
    "        if self.i == 1:\n",
    "            df.to_csv(self.file_path, mode='w', header=True)\n",
    "        else:\n",
    "            df.to_csv(self.file_path, mode='a', header=False)\n",
    "\n",
    "        # Increase iteration number\n",
    "        self.i += 1\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sklearn_pipeline(params, fill_na, sampling):\n",
    "    steps = []\n",
    "\n",
    "    # Impute\n",
    "    if type(fill_na) == int:\n",
    "        steps.append(('impute', SimpleImputer(strategy='constant', fill_value=fill_na)))\n",
    "    else:\n",
    "        steps.append(('impute', SimpleImputer(strategy=fill_na)))\n",
    "\n",
    "    # SMOTE\n",
    "    if sampling == 'smote':\n",
    "        steps.append(('smote', SMOTE(random_state=1)))\n",
    "        steps.append(('rf_clf', RandomForestClassifier(random_state=1, n_estimators=200, n_jobs=-1, **params)))\n",
    "        pipe = imbPipeline(steps=steps)\n",
    "\n",
    "    else:\n",
    "        steps.append(('rf_clf', RandomForestClassifier(random_state=1, n_estimators=200, n_jobs=-1, **params)))\n",
    "        pipe = Pipeline(steps=steps)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def objective(params, LogFile):\n",
    "\n",
    "    # Initialize\n",
    "    params_rf = {\n",
    "        'class_weight': params['class_weight'],\n",
    "        'criterion': params['criterion'],\n",
    "        'max_depth': None if params['max_depth'] == None else int(params['max_depth']),\n",
    "        'min_samples_split': int(params['min_samples_split']),\n",
    "        'min_samples_leaf': int(params['min_samples_split']),\n",
    "        'max_features': params['max_features'],\n",
    "        'max_leaf_nodes': None if params['max_leaf_nodes'] == None else int(params['max_leaf_nodes'])\n",
    "    }\n",
    "\n",
    "    # Create pipeline\n",
    "    pipe = create_sklearn_pipeline(params_rf, params['fill_na'], params['sampling'])\n",
    "\n",
    "    # Fit data & calculate CV score\n",
    "    scores = cross_val_score(pipe, cv=5, X=X_train, y=y_train, scoring=misclassification_cost, n_jobs=-1)\n",
    "    cv_mean_cost = -scores.mean()\n",
    "\n",
    "    # Write params & results to log\n",
    "    LogFile.write_log_file(cv_mean_cost, params, params_rf)\n",
    "\n",
    "    return cv_mean_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [05:33<17:23:18, 125.95s/trial, best loss: 10178.0]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "space = {\n",
    "    'fill_na': hp.choice('fill_na', [0, -1, -100, -10_000, -1_000_000, 'mean', 'most_frequent']),\n",
    "    'sampling': hp.choice('sampling', [None, 'smote']),\n",
    "    'class_weight': hp.choice('class_weight', ['balanced', 'balanced_subsample']),\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "    'max_depth': hp.choice('max_depth', [None, hp.quniform('max_depth_int', 2, 1000, 2)]),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 2, 500),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 2, 500),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2', None]),\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes', [None, hp.uniform('max_leaf_nodes_int', 100, 500)])\n",
    "}\n",
    "\n",
    "# Set up log file\n",
    "log_file = LogFile(r'./logs/log_rf_bayes_opt.csv')\n",
    "\n",
    "# Set up objective function & pass log file\n",
    "f_objective = partial(objective, LogFile=log_file)\n",
    "\n",
    "# Bayesian Optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=f_objective, space=space, max_evals=500, rstate=np.random.default_rng(1), algo=tpe.suggest, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Estimator - Plots & Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = create_sklearn_pipeline(best)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(pipe, cv=5, X=X_train, y=y_train, scoring=misclassification_cost, n_jobs=-1)\n",
    "print(-scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(y_true, y_pred, model_name, file_path, figsize=(10, 8)):\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Calculate ROC Curve & AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    area = auc(fpr, tpr)\n",
    "    plt.title('ROC Curve | %s | AUC = %0.5f' % (model_name, area))\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "\n",
    "    # Save & close plot\n",
    "    plt.plot(fpr, tpr)\n",
    "    fig.savefig(file_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def plot_precision_recall_auc(y_true, y_pred, model_name, file_path, figsize=(10, 8)):\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Calculate ROC Curve & AUC\n",
    "    pr, rc, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    area = auc(rc, pr)\n",
    "    plt.title('Precision-Recall Curve | %s | AUC = %0.5f' % (model_name, area))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "\n",
    "    # Save & close plot\n",
    "    plt.plot(rc, pr)\n",
    "    fig.savefig(file_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return area\n",
    "\n",
    "probs = pipe.predict_proba(X_train)\n",
    "\n",
    "plot_roc_auc(y_train.replace({'neg': 0, 'pos': 1}), probs[:, 1], 'Complement Naive Bayes', r'./results/cnb_roc.jpg');\n",
    "plot_precision_recall_auc(y_train.replace({'neg': 0, 'pos': 1}), probs[:, 1], 'Complement Naive Bayes', r'./results/cnb_pr.jpg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test = {\n",
    "    'n_estimators': 150,\n",
    "    'criterion': 'entropy',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 6,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'max_leaf_nodes': None,\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=150, n_jobs=-1, **params_test)\n",
    "\n",
    "steps = [\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('rf_clf', clf)\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps=steps)\n",
    "\n",
    "scores = cross_val_score(pipe, cv=3, X=X_train, y=y_train, scoring=misclassification_cost, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test run\n",
    "\n",
    "trials = Trials()\n",
    "space = {\n",
    "    'fill_na': hp.choice('fill_na', [0, 'mean', 'most_frequent']),\n",
    "    'sampling': hp.choice('sampling', [None, 'smote']),\n",
    "    'class_weight': hp.choice('class_weight', ['balanced', 'balanced_subsample']),\n",
    "    'max_depth': hp.choice('max_depth', [None, hp.quniform('max_depth_int', 2, 50, 2)]),\n",
    "    'max_leaf_nodes': hp.loguniform('max_leaf_nodes', 0.1, 10)\n",
    "}\n",
    "\n",
    "log_file = LogFile(r'./logs/log_rf_bayes_opt.csv')\n",
    "\n",
    "f_objective = partial(objective, LogFile=log_file)\n",
    "\n",
    "best = fmin(fn=f_objective, space=space, max_evals=10, rstate=np.random.default_rng(1), algo=tpe.suggest, trials=trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('xgb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb3f0c3a697a7256f3122816dd9f3ac634f951acfd855c57fce6592b991e3e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
