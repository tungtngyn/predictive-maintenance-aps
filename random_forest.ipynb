{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "This notebook fits a Random Forest model to the Scania Trucks Air Pressure System (APS) predictive maintenance dataset, obtained from [UCI's data repository](https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks). \n",
    "\n",
    "In the Logistic Regression & Naive Bayes notebooks, grid searches were used for hyperparameter tuning. However, significantly more computational resources are required to fit a Random Forest model. Thus, this notebook will use Bayesian Optimization in lieu of grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, make_scorer, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'./data/aps_failure_training_set_data_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df --> X_train & y_train\n",
    "X_train = df_train.drop('class', axis=1)\n",
    "y_train = df_train['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_misclassification_cost(y, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    return 10*fp + 500*fn\n",
    "\n",
    "\n",
    "misclassification_cost = make_scorer(\n",
    "    calc_misclassification_cost,\n",
    "    greater_is_better=False,\n",
    "    needs_proba=False,\n",
    "    needs_threshold=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogFile:\n",
    "    def __init__(self, file_path):\n",
    "        self.i = 1\n",
    "        self.file_path = file_path\n",
    "        pass\n",
    "\n",
    "    def write_log_file(self, cv_mean_cost, params, params_skl):\n",
    "        \n",
    "        # Initialize\n",
    "        dd = defaultdict(list)\n",
    "\n",
    "        # Combine hyperopt params & skl params\n",
    "        for d in (params, params_skl):\n",
    "            for key, value in d.items():\n",
    "                dd[key].append(value)\n",
    "\n",
    "        # Create df & add iteration / cost\n",
    "        df = pd.DataFrame(dd, index=['hyperopt', 'sklearn'])\n",
    "        df['Iteration'] = self.i\n",
    "        df['cv_mean_cost'] = cv_mean_cost\n",
    "\n",
    "        # Reorder & append to log file\n",
    "        df = df[['Iteration', 'cv_mean_cost', *dd.keys()]]\n",
    "\n",
    "        # Write header\n",
    "        if self.i == 1:\n",
    "            df.to_csv(self.file_path, mode='w', header=True)\n",
    "        else:\n",
    "            df.to_csv(self.file_path, mode='a', header=False)\n",
    "\n",
    "        # Increase iteration number\n",
    "        self.i += 1\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sklearn_pipeline(params, fill_na, sampling):\n",
    "    steps = []\n",
    "\n",
    "    # Impute\n",
    "    if type(fill_na) == int:\n",
    "        steps.append(('impute', SimpleImputer(strategy='constant', fill_value=fill_na)))\n",
    "    else:\n",
    "        steps.append(('impute', SimpleImputer(strategy=fill_na)))\n",
    "\n",
    "    # SMOTE\n",
    "    if sampling == 'smote':\n",
    "        steps.append(('smote', SMOTE(random_state=1)))\n",
    "        steps.append(('rf_clf', RandomForestClassifier(random_state=1, n_estimators=150, n_jobs=-1, **params)))\n",
    "        pipe = imbPipeline(steps=steps)\n",
    "\n",
    "    else:\n",
    "        steps.append(('rf_clf', RandomForestClassifier(random_state=1, n_estimators=150, n_jobs=-1, **params)))\n",
    "        pipe = Pipeline(steps=steps)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def objective(params, LogFile):\n",
    "\n",
    "    # Initialize\n",
    "    params_rf = {\n",
    "        'class_weight': params['class_weight'],\n",
    "        'criterion': params['criterion'],\n",
    "        'max_depth': None if params['max_depth'] == None else int(params['max_depth']),\n",
    "        'min_samples_split': int(params['min_samples_split']),\n",
    "        'min_samples_leaf': int(params['min_samples_split']),\n",
    "        'max_features': params['max_features'],\n",
    "        'max_leaf_nodes': None if params['max_leaf_nodes'] == None else int(params['max_leaf_nodes'])\n",
    "    }\n",
    "\n",
    "    # Create pipeline\n",
    "    pipe = create_sklearn_pipeline(params_rf, params['fill_na'], params['sampling'])\n",
    "\n",
    "    # Fit data & calculate CV score\n",
    "    scores = cross_val_score(pipe, cv=5, X=X_train, y=y_train, scoring=misclassification_cost, n_jobs=-1)\n",
    "    cv_mean_cost = -scores.mean()\n",
    "\n",
    "    # Write params & results to log\n",
    "    LogFile.write_log_file(cv_mean_cost, params, params_rf)\n",
    "\n",
    "    return cv_mean_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 80/300 [2:31:41<6:57:09, 113.77s/trial, best loss: 9142.0] \n",
      "CPU times: user 5.88 s, sys: 2.34 s, total: 8.22 s\n",
      "Wall time: 2h 31min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "space = {\n",
    "    'fill_na': hp.choice('fill_na', [0, -1, -10_000, 'most_frequent']),\n",
    "    'sampling': hp.choice('sampling', [None, 'smote']),\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced', 'balanced_subsample']),\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "    'max_depth': hp.choice('max_depth', [None, hp.quniform('max_depth_int', 2, 1000, 2)]),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 2, 500),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 2, 500),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2', None]),\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes', [None, hp.uniform('max_leaf_nodes_int', 100, 500)])\n",
    "}\n",
    "\n",
    "# Set up log file\n",
    "log_file = LogFile(r'./logs/log_rf_bayes_opt.csv')\n",
    "\n",
    "# Set up objective function & pass log file\n",
    "f_objective = partial(objective, LogFile=log_file)\n",
    "\n",
    "# Bayesian Optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=f_objective, space=space, early_stop_fn=no_progress_loss(40), max_evals=300, rstate=np.random.default_rng(1), algo=tpe.suggest, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 2,\n",
       " 'criterion': 2,\n",
       " 'fill_na': 0,\n",
       " 'max_depth': 1,\n",
       " 'max_depth_int': 926.0,\n",
       " 'max_features': 2,\n",
       " 'max_leaf_nodes': 1,\n",
       " 'max_leaf_nodes_int': 160.02443333483498,\n",
       " 'min_samples_leaf': 21.472890321167622,\n",
       " 'min_samples_split': 133.56709435734882,\n",
       " 'sampling': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>cv_mean_cost</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>criterion</th>\n",
       "      <th>fill_na</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyperopt</td>\n",
       "      <td>1</td>\n",
       "      <td>10090.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>190.596769</td>\n",
       "      <td>278.436732</td>\n",
       "      <td>174.454308</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>1</td>\n",
       "      <td>10090.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyperopt</td>\n",
       "      <td>2</td>\n",
       "      <td>10160.0</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.773527</td>\n",
       "      <td>379.293074</td>\n",
       "      <td>345.171999</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>2</td>\n",
       "      <td>10160.0</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyperopt</td>\n",
       "      <td>3</td>\n",
       "      <td>10638.0</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>entropy</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>445.517986</td>\n",
       "      <td>444.490862</td>\n",
       "      <td>437.961795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  Iteration  cv_mean_cost        class_weight criterion  \\\n",
       "0   hyperopt          1       10090.0            balanced  log_loss   \n",
       "1    sklearn          1       10090.0            balanced  log_loss   \n",
       "2   hyperopt          2       10160.0  balanced_subsample  log_loss   \n",
       "3    sklearn          2       10160.0  balanced_subsample  log_loss   \n",
       "4   hyperopt          3       10638.0  balanced_subsample   entropy   \n",
       "\n",
       "         fill_na  max_depth max_features  max_leaf_nodes  min_samples_leaf  \\\n",
       "0             -1        NaN         sqrt      190.596769        278.436732   \n",
       "1             -1        NaN         sqrt      190.000000        174.000000   \n",
       "2              0      992.0          NaN      106.773527        379.293074   \n",
       "3              0      992.0          NaN      106.000000        345.000000   \n",
       "4  most_frequent      106.0          NaN      445.517986        444.490862   \n",
       "\n",
       "   min_samples_split sampling  \n",
       "0         174.454308      NaN  \n",
       "1         174.000000      NaN  \n",
       "2         345.171999      NaN  \n",
       "3         345.000000      NaN  \n",
       "4         437.961795      NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = pd.read_csv(r'./logs/log_rf_bayes_opt.csv')\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>cv_mean_cost</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>criterion</th>\n",
       "      <th>fill_na</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_leaf_nodes</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>40</td>\n",
       "      <td>9142.0</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>hyperopt</td>\n",
       "      <td>40</td>\n",
       "      <td>9142.0</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.024433</td>\n",
       "      <td>21.472890</td>\n",
       "      <td>133.567094</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>hyperopt</td>\n",
       "      <td>41</td>\n",
       "      <td>9370.0</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>950.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145.032613</td>\n",
       "      <td>29.599696</td>\n",
       "      <td>139.585983</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>41</td>\n",
       "      <td>9370.0</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>950.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>hyperopt</td>\n",
       "      <td>25</td>\n",
       "      <td>9416.0</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-10000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>350.136162</td>\n",
       "      <td>490.666896</td>\n",
       "      <td>64.541277</td>\n",
       "      <td>smote</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Iteration  cv_mean_cost        class_weight criterion  \\\n",
       "79    sklearn         40        9142.0  balanced_subsample  log_loss   \n",
       "78   hyperopt         40        9142.0  balanced_subsample  log_loss   \n",
       "80   hyperopt         41        9370.0  balanced_subsample  log_loss   \n",
       "81    sklearn         41        9370.0  balanced_subsample  log_loss   \n",
       "48   hyperopt         25        9416.0            balanced  log_loss   \n",
       "\n",
       "          fill_na  max_depth max_features  max_leaf_nodes  min_samples_leaf  \\\n",
       "79              0      926.0          NaN      160.000000        133.000000   \n",
       "78              0      926.0          NaN      160.024433         21.472890   \n",
       "80  most_frequent      950.0          NaN      145.032613         29.599696   \n",
       "81  most_frequent      950.0          NaN      145.000000        139.000000   \n",
       "48         -10000       50.0         sqrt      350.136162        490.666896   \n",
       "\n",
       "    min_samples_split sampling  \n",
       "79         133.000000      NaN  \n",
       "78         133.567094      NaN  \n",
       "80         139.585983      NaN  \n",
       "81         139.000000      NaN  \n",
       "48          64.541277    smote  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.sort_values(by='cv_mean_cost').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Estimator - Plots & Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced_subsample',\n",
       " 'criterion': 'log_loss',\n",
       " 'fill_na': '0',\n",
       " 'max_depth': 926.0,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': 160.0,\n",
       " 'min_samples_leaf': 133.0,\n",
       " 'min_samples_split': 133.0,\n",
       " 'sampling': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.iloc[79].replace({np.nan: None}).drop(['Unnamed: 0', 'Iteration', 'cv_mean_cost']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9142.0\n"
     ]
    }
   ],
   "source": [
    "params_rf = log_df.iloc[79].replace({np.nan: None}).drop(['Unnamed: 0', 'Iteration', 'cv_mean_cost', 'fill_na', 'sampling']).to_dict()\n",
    "params_rf['max_depth'] = int(params_rf['max_depth'])\n",
    "params_rf['max_leaf_nodes'] = int(params_rf['max_leaf_nodes'])\n",
    "params_rf['min_samples_leaf'] = int(params_rf['min_samples_leaf'])\n",
    "params_rf['min_samples_split'] = int(params_rf['min_samples_split'])\n",
    "\n",
    "pipe = create_sklearn_pipeline(params_rf, 0, None)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(pipe, cv=5, X=X_train, y=y_train, scoring=misclassification_cost, n_jobs=-1)\n",
    "print(-scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(y_true, y_pred, model_name, file_path, figsize=(10, 8)):\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Calculate ROC Curve & AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    area = auc(fpr, tpr)\n",
    "    plt.title('ROC Curve | %s | AUC = %0.5f' % (model_name, area))\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "\n",
    "    # Save & close plot\n",
    "    plt.plot(fpr, tpr)\n",
    "    fig.savefig(file_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def plot_precision_recall_auc(y_true, y_pred, model_name, file_path, figsize=(10, 8)):\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # Calculate ROC Curve & AUC\n",
    "    pr, rc, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    area = auc(rc, pr)\n",
    "    plt.title('Precision-Recall Curve | %s | AUC = %0.5f' % (model_name, area))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "\n",
    "    # Save & close plot\n",
    "    plt.plot(rc, pr)\n",
    "    fig.savefig(file_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return area\n",
    "\n",
    "probs = pipe.predict_proba(X_train)\n",
    "\n",
    "plot_roc_auc(y_train.replace({'neg': 0, 'pos': 1}), probs[:, 1], 'Random Forest', r'./results/rf_roc.jpg');\n",
    "plot_precision_recall_auc(y_train.replace({'neg': 0, 'pos': 1}), probs[:, 1], 'Random Forest', r'./results/rf_pr.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./results/rf_roc.jpg)\n",
    "![image](./results/rf_pr.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassification Cost on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification Cost on Test Data: 12970\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(r'./data/aps_failure_test_set_data_only.csv')\n",
    "\n",
    "X_test = df_test.drop('class', axis=1)\n",
    "y_test = df_test['class']\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('Misclassification Cost on Test Data: %i' % calc_misclassification_cost(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Type 1 Faults: 547\n",
      "Number of Type 2 Faults: 15\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('Number of Type 1 Faults: %i' % fp)\n",
    "print('Number of Type 2 Faults: %i' % fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Random Forest model, tuned with Bayesian Optimization, performed better than both the ComplementNB and Logistic Regression models.\n",
    "\n",
    "* The Random Forest had more Type 1 faults than the Logistic Regression model, however, it had 7 less Type 2 faults (22 for LR model vs. 15 for the RF model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('xgb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb3f0c3a697a7256f3122816dd9f3ac634f951acfd855c57fce6592b991e3e66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
